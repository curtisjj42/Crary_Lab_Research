{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# pip install openslide-python pillow numpy matplotlib\n",
    "\n",
    "import os\n",
    "os.add_dll_directory(r'C:\\Users\\curti\\OpenSlide\\bin')\n",
    "\n",
    "import openslide\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filepath = r'C:\\Users\\curti\\PycharmProjects\\Crary_Lab\\slides\\41998.svs'\n",
    "\n",
    "# Open WSI file\n",
    "slide = openslide.OpenSlide(filepath)\n",
    "\n",
    "# Basic properties\n",
    "print(f\"Dimensions: {slide.dimensions}\")\n",
    "print(f\"Level count: {slide.level_count}\")\n",
    "print(f\"Level dimensions: {slide.level_dimensions}\")\n",
    "print(f\"Downsample factors: {slide.level_downsamples}\")\n",
    "\n",
    "# Get a thumbnail for visualization\n",
    "thumbnail = slide.get_thumbnail((1000, 1000))\n",
    "plt.imshow(thumbnail)\n",
    "plt.title(\"WSI Thumbnail\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_wsi(slide, tile_size=224, level=0, overlap=0):\n",
    "    \"\"\"\n",
    "    Tile a WSI into smaller patches\n",
    "    \n",
    "    Args:\n",
    "        slide: OpenSlide object\n",
    "        tile_size: Size of each tile (pixels)\n",
    "        level: Pyramid level to use (0 = highest resolution)\n",
    "        overlap: Overlap between tiles (pixels)\n",
    "    \n",
    "    Returns:\n",
    "        List of tiles with their coordinates\n",
    "    \"\"\"\n",
    "    # 1. Get dimensions at the specified pyramid level\n",
    "    width, height = slide.level_dimensions[level]\n",
    "    downsample = slide.level_downsamples[level]\n",
    "    \n",
    "    tiles = []\n",
    "    \n",
    "    # 2. Calculate stride (step size between tiles)\n",
    "    stride = tile_size - overlap\n",
    "    \n",
    "    # 3. Iterate through the slide in a grid pattern\n",
    "    for y in range(0, height, stride):\n",
    "        for x in range(0, width, stride):\n",
    "            # 4. Convert coordinates to level 0 (base resolution)\n",
    "            x_level0 = int(x * downsample)\n",
    "            y_level0 = int(y * downsample)\n",
    "            \n",
    "            # 5. Read the tile from the slide\n",
    "            tile = slide.read_region(\n",
    "                (x_level0, y_level0), \n",
    "                level, \n",
    "                (tile_size, tile_size)\n",
    "            )\n",
    "            \n",
    "            # 6. Store tile with metadata\n",
    "            tiles.append({\n",
    "                'image': tile.convert('RGB'),\n",
    "                'x': x_level0,\n",
    "                'y': y_level0,\n",
    "                'level': level\n",
    "            })\n",
    "    \n",
    "    return tiles\n",
    "\n",
    "\"\"\" Whole Slide Images are **massive** (often 100,000 × 100,000 pixels or larger). To handle this, they're stored as **image pyramids**:\n",
    "\n",
    "Level 0 (highest resolution):  100,000 × 100,000 pixels  (original scan)\n",
    "Level 1:                        50,000 × 50,000 pixels   (2× downsampled)\n",
    "Level 2:                        25,000 × 25,000 pixels   (4× downsampled)\n",
    "Level 3:                        12,500 × 12,500 pixels   (8× downsampled)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# pip install torch torchvision timm huggingface_hub\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load token and connect to HF\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)\n",
    "else:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN not found in environment variables\")\n",
    "\n",
    "# Load UNI model\n",
    "model = timm.create_model(\n",
    "    \"hf-hub:mahmoodLab/uni\",\n",
    "    pretrained=True,\n",
    "    init_values=1e-5,\n",
    "    dynamic_img_size=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Preprocessing\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Extract embeddings\n",
    "def extract_embeddings(tiles, model, transform, device):\n",
    "    \"\"\"Extract embeddings for all tiles\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tile_data in tiles:\n",
    "            tile = tile_data['image']\n",
    "            tile_tensor = transform(tile).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get embedding\n",
    "            embedding = model(tile_tensor)\n",
    "            embeddings.append({\n",
    "                'embedding': embedding.cpu().numpy(),\n",
    "                'x': tile_data['x'],\n",
    "                'y': tile_data['y']\n",
    "            })\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "# Test API access\n",
    "api = HfApi()\n",
    "\n",
    "# Check if you can access UNI\n",
    "try:\n",
    "    model_info = api.model_info(\"MahmoodLab/UNI\")\n",
    "    print(\"✓ Successfully accessed UNI model!\")\n",
    "    print(f\"  Model ID: {model_info.modelId}\")\n",
    "    print(f\"  Downloads: {model_info.downloads}\")\n",
    "    print(f\"  Gated: {model_info.gated}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"✗ Cannot access UNI model\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Visit https://huggingface.co/MahmoodLab/UNI\")\n",
    "    print(\"2. Click 'Request access' if you haven't\")\n",
    "    print(\"3. Wait for approval email\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
