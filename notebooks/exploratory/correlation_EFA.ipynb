{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDS Analysis: Correlations and EFA (Post‑Cleaning)\n",
    "\n",
    "This notebook focuses on analysis steps only. Please run the separate notebook\n",
    "\"uds_data_cleaning\" first to build the variable catalog, clean/align the dataset,\n",
    "and generate the parquet files used here.\n",
    "\n",
    "Inputs expected from uds_data_cleaning:\n",
    "- outputs/uds_extraction/mmse_only.parquet\n",
    "- outputs/uds_extraction/moca_only.parquet\n",
    "\n",
    "Outputs produced here include correlation matrices/plots, filtered correlation artifacts,\n",
    "and EFA loadings/communalities and plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "OUT_DIR = '../../outputs/uds_extraction'\n",
    "PLOT_HEATMAP = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root to sys.path so we can import from 'src' if needed\n",
    "project_root = next((p for p in [Path.cwd()] + list(Path.cwd().parents) if (p / 'src').exists()), None)\n",
    "if project_root and str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "out_dir = Path(OUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_dir.as_posix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-cleaned subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmse_only_path = out_dir / 'mmse_only.parquet'\n",
    "moca_only_path = out_dir / 'moca_only.parquet'\n",
    "df_mmse_only = pd.read_parquet(mmse_only_path)\n",
    "df_moca_only = pd.read_parquet(moca_only_path)\n",
    "print(f\"Loaded: {mmse_only_path} ({df_mmse_only.shape}) | {moca_only_path} ({df_moca_only.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis for MMSE-only and MOCA-only sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _prepare_numeric(df: pd.DataFrame, drop_cols=None) -> pd.DataFrame:\n",
    "    drop_cols = set(drop_cols or [])\n",
    "    # Select numeric columns only\n",
    "    num = df.select_dtypes(include=[\"number\"]).copy()\n",
    "    # Drop known indicator/ID columns if present\n",
    "    for col in [\"has_MMSE\", \"has_MOCA\"]:\n",
    "        if col in num.columns:\n",
    "            drop_cols.add(col)\n",
    "    num = num.drop(columns=[c for c in drop_cols if c in num.columns], errors=\"ignore\")\n",
    "    # Drop columns that are all NA or constant\n",
    "    non_na = num.dropna(axis=1, how=\"all\")\n",
    "    nunique = non_na.nunique(dropna=True)\n",
    "    non_constant = non_na.loc[:, nunique > 1]\n",
    "    # Further drop near-zero-variance (NZV) columns to aid factor convergence\n",
    "    # Criteria: very small variance OR one level dominates (>= 99% same value)\n",
    "    if non_constant.shape[1] == 0:\n",
    "        return non_constant\n",
    "    variances = non_constant.var(ddof=0)\n",
    "    var_mask = variances > 1e-6\n",
    "    # Dominant level frequency\n",
    "    freq_mask = []\n",
    "    for c in non_constant.columns:\n",
    "        vc = non_constant[c].value_counts(normalize=True, dropna=True)\n",
    "        max_prop = float(vc.iloc[0]) if len(vc) else 1.0\n",
    "        freq_mask.append(max_prop < 0.99)\n",
    "    freq_mask = pd.Series(freq_mask, index=non_constant.columns)\n",
    "    keep_mask = var_mask & freq_mask\n",
    "    nzv_filtered = non_constant.loc[:, keep_mask]\n",
    "    return nzv_filtered\n",
    "\n",
    "def compute_and_save_correlations(df: pd.DataFrame, label: str, out_dir: Path,\n",
    "                                  methods=(\"pearson\", \"spearman\"),\n",
    "                                  plot=PLOT_HEATMAP) -> None:\n",
    "    data = _prepare_numeric(df)\n",
    "    if data.shape[1] < 2:\n",
    "        print(f\"[{label}] Not enough numeric columns for correlation (found {data.shape[1]}). Skipping.\")\n",
    "        return\n",
    "    for method in methods:\n",
    "        corr = data.corr(method=method)\n",
    "        out_csv = out_dir / f\"corr_{label}_{method}.csv\"\n",
    "        corr.to_csv(out_csv)\n",
    "        print(f\"[{label}] Saved {method} correlation matrix to {out_csv}\")\n",
    "        if plot:\n",
    "            plt.figure(figsize=(max(8, min(20, 0.35 * corr.shape[1])),\n",
    "                               max(6, min(20, 0.35 * corr.shape[0]))))\n",
    "            sns.heatmap(corr, cmap=\"vlag\", center=0, square=True,\n",
    "                        cbar_kws={\"shrink\": 0.6}, linewidths=0.3)\n",
    "            plt.title(f\"{label.upper()} — {method.title()} correlation\")\n",
    "            plt.tight_layout()\n",
    "            img_path = out_dir / f\"corr_{label}_{method}.png\"\n",
    "            plt.savefig(img_path, dpi=200)\n",
    "            plt.close()\n",
    "            print(f\"[{label}] Saved {method} correlation heatmap to {img_path}\")\n",
    "\n",
    "# Run for each set separately\n",
    "compute_and_save_correlations(df_mmse_only, label=\"mmse_only\", out_dir=out_dir)\n",
    "compute_and_save_correlations(df_moca_only, label=\"moca_only\", out_dir=out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation filter and Exploratory Factor Analysis (EFA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "def _missing_rate_per_column(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isna().mean()\n",
    "\n",
    "def correlation_filter(df: pd.DataFrame,\n",
    "                       method: str = \"spearman\",\n",
    "                       threshold: float = 0.95,\n",
    "                       prefer_keep: Tuple[str, ...] = (),\n",
    "                       drop_cols: Tuple[str, ...] = ()) -> Tuple[pd.DataFrame, List[str], pd.DataFrame]:\n",
    "    \"\"\"Filter out one variable from any pair with |corr| >= threshold.\n",
    "\n",
    "    Tie-breaker rules:\n",
    "      1) Keep variables listed in prefer_keep if involved in a tie.\n",
    "      2) Otherwise drop the one with higher missingness rate.\n",
    "      3) If equal, drop the one that is later alphabetically.\n",
    "\n",
    "    Returns: (filtered_df, dropped_columns, corr_matrix_of_kept)\n",
    "    \"\"\"\n",
    "    data = _prepare_numeric(df, drop_cols=drop_cols)\n",
    "    if data.shape[1] < 2:\n",
    "        return data, [], data.corr(method=method)\n",
    "\n",
    "    corr = data.corr(method=method).abs()\n",
    "    np.fill_diagonal(corr.values, 0.0)\n",
    "    miss = _missing_rate_per_column(data)\n",
    "\n",
    "    to_drop: set = set()\n",
    "    keep_set: set = set(prefer_keep)\n",
    "    cols = list(data.columns)\n",
    "\n",
    "    # Create list of pairs above threshold\n",
    "    pairs: List[Tuple[str, str, float]] = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            r = corr.iloc[i, j]\n",
    "            if r >= threshold:\n",
    "                pairs.append((cols[i], cols[j], r))\n",
    "\n",
    "    # Sort pairs by strength descending so we handle strongest first\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    for a, b, _ in pairs:\n",
    "        if a in to_drop or b in to_drop:\n",
    "            continue\n",
    "        # Decide which to drop\n",
    "        if a in keep_set and b in keep_set:\n",
    "            # both preferred, fall through to missingness\n",
    "            pass\n",
    "        elif a in keep_set:\n",
    "            to_drop.add(b)\n",
    "            continue\n",
    "        elif b in keep_set:\n",
    "            to_drop.add(a)\n",
    "            continue\n",
    "\n",
    "        ma = float(miss.get(a, 0.0))\n",
    "        mb = float(miss.get(b, 0.0))\n",
    "        if ma > mb:\n",
    "            to_drop.add(a)\n",
    "        elif mb > ma:\n",
    "            to_drop.add(b)\n",
    "        else:\n",
    "            # Alphabetical tiebreaker: drop later\n",
    "            to_drop.add(max(a, b))\n",
    "\n",
    "    kept_cols = [c for c in cols if c not in to_drop]\n",
    "    filtered = data[kept_cols].copy()\n",
    "    corr_kept = filtered.corr(method=method)\n",
    "    return filtered, sorted(list(to_drop)), corr_kept\n",
    "\n",
    "def _standardize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df - df.mean()) / df.std(ddof=0)\n",
    "\n",
    "def _eigenvalues_from_corr(df: pd.DataFrame) -> np.ndarray:\n",
    "    c = df.corr(method=\"pearson\").fillna(0)\n",
    "    vals, _ = np.linalg.eigh(c.values)\n",
    "    return np.sort(vals)[::-1]\n",
    "\n",
    "def varimax(Phi: np.ndarray, gamma: float = 1.0, q: int = 20, tol: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Varimax rotation of loadings matrix Phi (features x factors).\"\"\"\n",
    "    p, k = Phi.shape\n",
    "    R = np.eye(k)\n",
    "    d = 0\n",
    "    for i in range(q):\n",
    "        d_old = d\n",
    "        Lambda = Phi @ R\n",
    "        u, s, vh = np.linalg.svd(Phi.T @ (Lambda**3 - (gamma / p) * (Lambda @ np.diag(np.diag(Lambda.T @ Lambda)))))\n",
    "        R = u @ vh\n",
    "        d = s.sum()\n",
    "        if d_old != 0 and d / d_old < 1 + tol:\n",
    "            break\n",
    "    return Phi @ R\n",
    "\n",
    "def promax(Phi: np.ndarray, kappa: float = 4.0) -> tuple:\n",
    "    \"\"\"Promax (oblique) rotation of loadings matrix Phi.\n",
    "    Returns (Lambda, Phi_f), where Lambda are oblique-rotated loadings and\n",
    "    Phi_f is the factor correlation matrix.\n",
    "    \"\"\"\n",
    "    # Start from an orthogonal varimax solution\n",
    "    L = varimax(Phi)\n",
    "    # Target matrix U by raising to power kappa with sign\n",
    "    U = np.sign(L) * (np.abs(L) ** kappa)\n",
    "    # Regress U on L to find transformation P\n",
    "    # P = (L'L)^{-1} L'U  (stable via pinv)\n",
    "    P = np.linalg.pinv(L) @ U\n",
    "    Lambda = L @ P\n",
    "    # Factor correlation matrix Phi_f = (P^{-1})'(P^{-1})\n",
    "    try:\n",
    "        Pinv = np.linalg.inv(P)\n",
    "    except np.linalg.LinAlgError:\n",
    "        Pinv = np.linalg.pinv(P)\n",
    "    Phi_f = Pinv.T @ Pinv\n",
    "    return Lambda, Phi_f\n",
    "\n",
    "def run_efa(df: pd.DataFrame,\n",
    "            label: str,\n",
    "            out_dir: Path,\n",
    "            rotation: str = \"varimax\",\n",
    "            kaiser: bool = True,\n",
    "            n_factors: int = None) -> Dict[str, object]:\n",
    "    \"\"\"Run EFA on standardized numeric data with listwise deletion.\n",
    "    Chooses number of factors by Kaiser criterion if n_factors is None and kaiser is True.\n",
    "    Saves artifacts to out_dir with prefix 'efa_{label}_*'.\n",
    "    \"\"\"\n",
    "    X = _prepare_numeric(df)\n",
    "    # Listwise deletion\n",
    "    X = X.dropna(axis=0, how=\"any\")\n",
    "    if X.shape[1] < 2 or X.shape[0] < 10:\n",
    "        print(f\"[{label}] Not enough data for EFA. Observations={X.shape[0]}, Vars={X.shape[1]}\")\n",
    "        return {}\n",
    "\n",
    "    # Standardize\n",
    "    Z = _standardize(X)\n",
    "\n",
    "    # Determine number of factors\n",
    "    if n_factors is None:\n",
    "        eigvals = _eigenvalues_from_corr(Z)\n",
    "        if kaiser:\n",
    "            n_factors = int((eigvals > 1.0).sum())\n",
    "        if not kaiser or n_factors < 1:\n",
    "            n_factors = max(1, min(6, X.shape[1] // 3))\n",
    "    # Scree plot\n",
    "    eigvals = _eigenvalues_from_corr(Z)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(eigvals)+1), eigvals, marker='o')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.title(f'{label.upper()} Scree Plot')\n",
    "    plt.tight_layout()\n",
    "    scree_path = out_dir / f'efa_{label}_scree.png'\n",
    "    plt.savefig(scree_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[{label}] Saved scree plot to {scree_path}\")\n",
    "\n",
    "    # Fit FactorAnalysis (ML) and rotate\n",
    "    fa = FactorAnalysis(n_components=n_factors, rotation=None)\n",
    "    fa.fit(Z.values)\n",
    "    loadings = fa.components_.T  # features x factors\n",
    "    factor_corr = None\n",
    "    rot = (rotation or \"\").lower()\n",
    "    if rot == \"varimax\":\n",
    "        loadings = varimax(loadings)\n",
    "    elif rot == \"promax\":\n",
    "        loadings, factor_corr = promax(loadings)\n",
    "\n",
    "    # Summaries\n",
    "    features = list(Z.columns)\n",
    "    loadings_df = pd.DataFrame(loadings, index=features, columns=[f\"F{i+1}\" for i in range(loadings.shape[1])])\n",
    "    uniqueness = getattr(fa, 'noise_variance_', np.maximum(0.0, 1.0 - (loadings**2).sum(axis=1)))\n",
    "    communalities = 1.0 - uniqueness\n",
    "    comm_df = pd.DataFrame({\"communality\": communalities, \"uniqueness\": uniqueness}, index=features)\n",
    "\n",
    "    # Save artifacts\n",
    "    loadings_csv = out_dir / f\"efa_{label}_loadings.csv\"\n",
    "    loadings_df.to_csv(loadings_csv)\n",
    "    print(f\"[{label}] Saved loadings to {loadings_csv}\")\n",
    "\n",
    "    comm_csv = out_dir / f\"efa_{label}_communalities.csv\"\n",
    "    comm_df.to_csv(comm_csv)\n",
    "    print(f\"[{label}] Saved communalities/uniqueness to {comm_csv}\")\n",
    "\n",
    "    # Save factor correlation matrix for oblique rotations\n",
    "    if factor_corr is not None:\n",
    "        phi_df = pd.DataFrame(factor_corr, index=loadings_df.columns, columns=loadings_df.columns)\n",
    "        phi_csv = out_dir / f\"efa_{label}_factor_correlation.csv\"\n",
    "        phi_df.to_csv(phi_csv)\n",
    "        print(f\"[{label}] Saved factor correlation matrix to {phi_csv}\")\n",
    "\n",
    "    # Heatmap of loadings\n",
    "    plt.figure(figsize=(max(6, 0.5 * loadings_df.shape[1] + 4), max(6, 0.25 * loadings_df.shape[0] + 2)))\n",
    "    sns.heatmap(loadings_df, cmap=\"coolwarm\", center=0, cbar_kws={\"shrink\": 0.6})\n",
    "    plt.title(f\"{label.upper()} Factor Loadings ({n_factors} factors)\")\n",
    "    plt.tight_layout()\n",
    "    loadings_png = out_dir / f\"efa_{label}_loadings.png\"\n",
    "    plt.savefig(loadings_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[{label}] Saved loadings heatmap to {loadings_png}\")\n",
    "\n",
    "    return {\n",
    "        \"n_factors\": n_factors,\n",
    "        \"loadings\": loadings_df,\n",
    "        \"communalities\": comm_df,\n",
    "        \"scree\": scree_path,\n",
    "        \"factor_correlation\": factor_corr,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation filter (separate step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_METHOD = \"spearman\"\n",
    "CF_THRESHOLD = 0.95\n",
    "\n",
    "filtered_mmse, dropped_mmse, corr_kept_mmse = correlation_filter(\n",
    "    df_mmse_only, method=CF_METHOD, threshold=CF_THRESHOLD\n",
    ")\n",
    "filtered_moca, dropped_moca, corr_kept_moca = correlation_filter(\n",
    "    df_moca_only, method=CF_METHOD, threshold=CF_THRESHOLD\n",
    ")\n",
    "\n",
    "# Save correlation of kept variables and plots\n",
    "for label, corr_kept, dropped in [\n",
    "    (\"mmse_only\", corr_kept_mmse, dropped_mmse),\n",
    "    (\"moca_only\", corr_kept_moca, dropped_moca),\n",
    "]:\n",
    "    out_csv = out_dir / f\"corr_{label}_filtered_{CF_METHOD}.csv\"\n",
    "    corr_kept.to_csv(out_csv)\n",
    "    print(f\"[{label}] Saved filtered correlation matrix to {out_csv}\")\n",
    "    plt.figure(figsize=(max(8, min(20, 0.35 * corr_kept.shape[1])), max(6, min(20, 0.35 * corr_kept.shape[0]))))\n",
    "    sns.heatmap(corr_kept, cmap=\"vlag\", center=0, square=True, cbar_kws={\"shrink\": 0.6}, linewidths=0.3)\n",
    "    plt.title(f\"{label.upper()} — filtered ({CF_METHOD}) correlation\")\n",
    "    plt.tight_layout()\n",
    "    img_path = out_dir / f\"corr_{label}_filtered_{CF_METHOD}.png\"\n",
    "    plt.savefig(img_path, dpi=200)\n",
    "    plt.close()\n",
    "    if dropped:\n",
    "        print(f\"[{label}] Dropped due to high correlation (|r|>={CF_THRESHOLD}): {', '.join(dropped)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Factor Analysis (separate step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_efa(filtered_mmse, label=\"mmse_only\", out_dir=out_dir, rotation=\"promax\", kaiser=True, n_factors=None)\n",
    "run_efa(filtered_moca, label=\"moca_only\", out_dir=out_dir, rotation=\"promax\", kaiser=True, n_factors=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
