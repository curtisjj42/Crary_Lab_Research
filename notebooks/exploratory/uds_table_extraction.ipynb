{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDS Table Extraction and Dataset Cleaning (Clean Notebook)\n",
    "\n",
    "This notebook focuses on extracting the C1/C2 Neuropsych Battery variable catalog from the UDS PDF and aligning the investigator CSV to those variables.\n",
    "\n",
    "Outputs saved to the configured output directory include:\n",
    "- `variable_catalog.csv`\n",
    "- `cleaned_subset.parquet` (only catalog variables)\n",
    "- `availability_summary.csv` (column-wise non-missing counts)\n",
    "- `stats.txt` (empty-rows summary)\n",
    "- Optional: `availability_heatmap.png`\n",
    "\n",
    "Requirements: `pandas`, `pdfplumber`, `matplotlib`, `seaborn` (for optional heatmap).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CSV_PATH = '../../data-files/investigator_nacc67.csv'\n",
    "PDF_PATH = '../../data-files/rdd_uds.pdf'\n",
    "PAGE_RANGE = (23, 27)  # inclusive zero-based pages for C1/C2 tables\n",
    "OUT_DIR = '../../outputs/uds_extraction'\n",
    "MMSE_COLS = ['NACCMMSE']  # extend if needed\n",
    "MOCA_COLS = ['NACCMOCA']  # extend if needed\n",
    "PLOT_HEATMAP = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Add the project root to sys.path so we can import from 'src'\n",
    "# This searches for the 'src' folder in the current directory or its parents\n",
    "project_root = next((p for p in [Path.cwd()] + list(Path.cwd().parents) if (p / 'src').exists()), None)\n",
    "if project_root and str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src.data.uds_extraction import (\n",
    "    build_variable_catalog,\n",
    "    load_nacc_csv,\n",
    "    align_dataset_to_catalog,\n",
    "    compute_empty_rows_mask,\n",
    "    plot_availability_heatmap,\n",
    ")\n",
    "\n",
    "out_dir = Path(OUT_DIR)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_dir.as_posix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build variable catalog from PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = build_variable_catalog(PDF_PATH, PAGE_RANGE)\n",
    "catalog_path = out_dir / 'variable_catalog.csv'\n",
    "catalog.to_csv(catalog_path, index=False)\n",
    "catalog.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV and align to catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_nacc_csv(CSV_PATH)\n",
    "cleaned, availability = align_dataset_to_catalog(\n",
    "    df, catalog, mmse_cols=MMSE_COLS, moca_cols=MOCA_COLS\n",
    ")\n",
    "cleaned_path = out_dir / 'cleaned_subset.parquet'\n",
    "availability_path = out_dir / 'availability_summary.csv'\n",
    "\n",
    "# 1. Automatically convert generic objects to best possible types (Int64, Float64, String)\n",
    "#    This prepares your numeric columns correctly for correlation analysis.\n",
    "cleaned = cleaned.convert_dtypes()\n",
    "\n",
    "# 2. For any columns that remain 'object' (likely mixed text/numbers), convert to String.\n",
    "#    This fixes the 'ArrowTypeError' by ensuring a valid format for Parquet\n",
    "#    without losing data. You can still cast these to numeric later if needed.\n",
    "for col in cleaned.select_dtypes(include=['object']).columns:\n",
    "    cleaned[col] = cleaned[col].astype(\"string\")\n",
    "\n",
    "cleaned.to_parquet(cleaned_path, index=False)\n",
    "availability.to_csv(availability_path, index=False)\n",
    "cleaned.shape, availability.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty-rows statistics and optional heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_mask = compute_empty_rows_mask(cleaned)\n",
    "stats_txt = (\n",
    "    f'Rows total: {len(cleaned)}\\n'\n",
    "    f'Completely empty (all -4/NaN): {int(empty_mask.sum())}\\n'\n",
    "    f'With some data: {int((~empty_mask).sum())}\\n'\n",
    ")\n",
    "(out_dir / 'stats.txt').write_text(stats_txt)\n",
    "print(stats_txt)\n",
    "if PLOT_HEATMAP:\n",
    "    plot_availability_heatmap(cleaned, out_path=str(out_dir / 'availability_heatmap.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick previews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cleaned)\n",
    "display(availability)\n",
    "# End of notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into MMSE-only and MOCA-only (XOR) and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows where exactly one of has_MMSE / has_MOCA is True (XOR)\n",
    "xor_mask = cleaned[\"has_MMSE\"] ^ cleaned[\"has_MOCA\"]\n",
    "filtered = cleaned.loc[xor_mask].copy()\n",
    "\n",
    "# Split into two sets\n",
    "df_mmse_only = filtered.loc[filtered[\"has_MMSE\"]].copy()\n",
    "df_moca_only = filtered.loc[filtered[\"has_MOCA\"]].copy()\n",
    "\n",
    "# Save\n",
    "mmse_only_path = out_dir / 'mmse_only.parquet'\n",
    "moca_only_path = out_dir / 'moca_only.parquet'\n",
    "df_mmse_only.to_parquet(mmse_only_path, index=False)\n",
    "df_moca_only.to_parquet(moca_only_path, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Saved MMSE-only rows: {len(df_mmse_only)} to {mmse_only_path}\\n\"\n",
    "    f\"Saved MOCA-only rows: {len(df_moca_only)} to {moca_only_path}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation analysis for MMSE-only and MOCA-only sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _prepare_numeric(df: pd.DataFrame, drop_cols=None) -> pd.DataFrame:\n",
    "    drop_cols = set(drop_cols or [])\n",
    "    # Select numeric columns only\n",
    "    num = df.select_dtypes(include=[\"number\"]).copy()\n",
    "    # Drop known indicator/ID columns if present\n",
    "    for col in [\"has_MMSE\", \"has_MOCA\"]:\n",
    "        if col in num.columns:\n",
    "            drop_cols.add(col)\n",
    "    num = num.drop(columns=[c for c in drop_cols if c in num.columns], errors=\"ignore\")\n",
    "    # Drop columns that are all NA or constant\n",
    "    non_na = num.dropna(axis=1, how=\"all\")\n",
    "    nunique = non_na.nunique(dropna=True)\n",
    "    non_constant = non_na.loc[:, nunique > 1]\n",
    "    # Further drop near-zero-variance (NZV) columns to aid factor convergence\n",
    "    # Criteria: very small variance OR one level dominates (>= 99% same value)\n",
    "    if non_constant.shape[1] == 0:\n",
    "        return non_constant\n",
    "    variances = non_constant.var(ddof=0)\n",
    "    var_mask = variances > 1e-6\n",
    "    # Dominant level frequency\n",
    "    freq_mask = []\n",
    "    for c in non_constant.columns:\n",
    "        vc = non_constant[c].value_counts(normalize=True, dropna=True)\n",
    "        max_prop = float(vc.iloc[0]) if len(vc) else 1.0\n",
    "        freq_mask.append(max_prop < 0.99)\n",
    "    freq_mask = pd.Series(freq_mask, index=non_constant.columns)\n",
    "    keep_mask = var_mask & freq_mask\n",
    "    nzv_filtered = non_constant.loc[:, keep_mask]\n",
    "    return nzv_filtered\n",
    "\n",
    "def compute_and_save_correlations(df: pd.DataFrame, label: str, out_dir: Path,\n",
    "                                  methods=(\"pearson\", \"spearman\"),\n",
    "                                  plot=PLOT_HEATMAP) -> None:\n",
    "    data = _prepare_numeric(df)\n",
    "    if data.shape[1] < 2:\n",
    "        print(f\"[{label}] Not enough numeric columns for correlation (found {data.shape[1]}). Skipping.\")\n",
    "        return\n",
    "    for method in methods:\n",
    "        corr = data.corr(method=method)\n",
    "        out_csv = out_dir / f\"corr_{label}_{method}.csv\"\n",
    "        corr.to_csv(out_csv)\n",
    "        print(f\"[{label}] Saved {method} correlation matrix to {out_csv}\")\n",
    "        if plot:\n",
    "            plt.figure(figsize=(max(8, min(20, 0.35 * corr.shape[1])),\n",
    "                               max(6, min(20, 0.35 * corr.shape[0]))))\n",
    "            sns.heatmap(corr, cmap=\"vlag\", center=0, square=True,\n",
    "                        cbar_kws={\"shrink\": 0.6}, linewidths=0.3)\n",
    "            plt.title(f\"{label.upper()} — {method.title()} correlation\")\n",
    "            plt.tight_layout()\n",
    "            img_path = out_dir / f\"corr_{label}_{method}.png\"\n",
    "            plt.savefig(img_path, dpi=200)\n",
    "            plt.close()\n",
    "            print(f\"[{label}] Saved {method} correlation heatmap to {img_path}\")\n",
    "\n",
    "# Run for each set separately\n",
    "compute_and_save_correlations(df_mmse_only, label=\"mmse_only\", out_dir=out_dir)\n",
    "compute_and_save_correlations(df_moca_only, label=\"moca_only\", out_dir=out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation filter and Exploratory Factor Analysis (EFA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "def _missing_rate_per_column(df: pd.DataFrame) -> pd.Series:\n",
    "    return df.isna().mean()\n",
    "\n",
    "def correlation_filter(df: pd.DataFrame,\n",
    "                       method: str = \"spearman\",\n",
    "                       threshold: float = 0.95,\n",
    "                       prefer_keep: Tuple[str, ...] = (),\n",
    "                       drop_cols: Tuple[str, ...] = ()) -> Tuple[pd.DataFrame, List[str], pd.DataFrame]:\n",
    "    \"\"\"Filter out one variable from any pair with |corr| >= threshold.\n",
    "\n",
    "    Tie-breaker rules:\n",
    "      1) Keep variables listed in prefer_keep if involved in a tie.\n",
    "      2) Otherwise drop the one with higher missingness rate.\n",
    "      3) If equal, drop the one that is later alphabetically.\n",
    "\n",
    "    Returns: (filtered_df, dropped_columns, corr_matrix_of_kept)\n",
    "    \"\"\"\n",
    "    data = _prepare_numeric(df, drop_cols=drop_cols)\n",
    "    if data.shape[1] < 2:\n",
    "        return data, [], data.corr(method=method)\n",
    "\n",
    "    corr = data.corr(method=method).abs()\n",
    "    np.fill_diagonal(corr.values, 0.0)\n",
    "    miss = _missing_rate_per_column(data)\n",
    "\n",
    "    to_drop: set = set()\n",
    "    keep_set: set = set(prefer_keep)\n",
    "    cols = list(data.columns)\n",
    "\n",
    "    # Create list of pairs above threshold\n",
    "    pairs: List[Tuple[str, str, float]] = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            r = corr.iloc[i, j]\n",
    "            if r >= threshold:\n",
    "                pairs.append((cols[i], cols[j], r))\n",
    "\n",
    "    # Sort pairs by strength descending so we handle strongest first\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    for a, b, _ in pairs:\n",
    "        if a in to_drop or b in to_drop:\n",
    "            continue\n",
    "        # Decide which to drop\n",
    "        if a in keep_set and b in keep_set:\n",
    "            # both preferred, fall through to missingness\n",
    "            pass\n",
    "        elif a in keep_set:\n",
    "            to_drop.add(b)\n",
    "            continue\n",
    "        elif b in keep_set:\n",
    "            to_drop.add(a)\n",
    "            continue\n",
    "\n",
    "        ma = float(miss.get(a, 0.0))\n",
    "        mb = float(miss.get(b, 0.0))\n",
    "        if ma > mb:\n",
    "            to_drop.add(a)\n",
    "        elif mb > ma:\n",
    "            to_drop.add(b)\n",
    "        else:\n",
    "            # Alphabetical tiebreaker: drop later\n",
    "            to_drop.add(max(a, b))\n",
    "\n",
    "    kept_cols = [c for c in cols if c not in to_drop]\n",
    "    filtered = data[kept_cols].copy()\n",
    "    corr_kept = filtered.corr(method=method)\n",
    "    return filtered, sorted(list(to_drop)), corr_kept\n",
    "\n",
    "def _standardize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df - df.mean()) / df.std(ddof=0)\n",
    "\n",
    "def _eigenvalues_from_corr(df: pd.DataFrame) -> np.ndarray:\n",
    "    c = df.corr(method=\"pearson\").fillna(0)\n",
    "    vals, _ = np.linalg.eigh(c.values)\n",
    "    return np.sort(vals)[::-1]\n",
    "\n",
    "def varimax(Phi: np.ndarray, gamma: float = 1.0, q: int = 20, tol: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Varimax rotation of loadings matrix Phi (features x factors).\"\"\"\n",
    "    p, k = Phi.shape\n",
    "    R = np.eye(k)\n",
    "    d = 0\n",
    "    for i in range(q):\n",
    "        d_old = d\n",
    "        Lambda = Phi @ R\n",
    "        u, s, vh = np.linalg.svd(Phi.T @ (Lambda**3 - (gamma / p) * (Lambda @ np.diag(np.diag(Lambda.T @ Lambda)))))\n",
    "        R = u @ vh\n",
    "        d = s.sum()\n",
    "        if d_old != 0 and d / d_old < 1 + tol:\n",
    "            break\n",
    "    return Phi @ R\n",
    "\n",
    "def run_efa(df: pd.DataFrame,\n",
    "            label: str,\n",
    "            out_dir: Path,\n",
    "            rotation: str = \"varimax\",\n",
    "            kaiser: bool = True,\n",
    "            n_factors: int = None) -> Dict[str, object]:\n",
    "    \"\"\"Run EFA on standardized numeric data with listwise deletion.\n",
    "    Chooses number of factors by Kaiser criterion if n_factors is None and kaiser is True.\n",
    "    Saves artifacts to out_dir with prefix 'efa_{label}_*'.\n",
    "    \"\"\"\n",
    "    X = _prepare_numeric(df)\n",
    "    # Listwise deletion\n",
    "    X = X.dropna(axis=0, how=\"any\")\n",
    "    if X.shape[1] < 2 or X.shape[0] < 10:\n",
    "        print(f\"[{label}] Not enough data for EFA. Observations={X.shape[0]}, Vars={X.shape[1]}\")\n",
    "        return {}\n",
    "\n",
    "    # Standardize\n",
    "    Z = _standardize(X)\n",
    "\n",
    "    # Determine number of factors\n",
    "    if n_factors is None:\n",
    "        eigvals = _eigenvalues_from_corr(Z)\n",
    "        if kaiser:\n",
    "            n_factors = int((eigvals > 1.0).sum())\n",
    "        if not kaiser or n_factors < 1:\n",
    "            n_factors = max(1, min(6, X.shape[1] // 3))\n",
    "    # Scree plot\n",
    "    eigvals = _eigenvalues_from_corr(Z)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(1, len(eigvals)+1), eigvals, marker='o')\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.title(f'{label.upper()} Scree Plot')\n",
    "    plt.tight_layout()\n",
    "    scree_path = out_dir / f'efa_{label}_scree.png'\n",
    "    plt.savefig(scree_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[{label}] Saved scree plot to {scree_path}\")\n",
    "\n",
    "    # Fit FactorAnalysis (ML) and rotate\n",
    "    fa = FactorAnalysis(n_components=n_factors, rotation=None)\n",
    "    fa.fit(Z.values)\n",
    "    loadings = fa.components_.T  # features x factors\n",
    "    if rotation == \"varimax\":\n",
    "        loadings = varimax(loadings)\n",
    "\n",
    "    # Summaries\n",
    "    features = list(Z.columns)\n",
    "    loadings_df = pd.DataFrame(loadings, index=features, columns=[f\"F{i+1}\" for i in range(loadings.shape[1])])\n",
    "    uniqueness = getattr(fa, 'noise_variance_', np.maximum(0.0, 1.0 - (loadings**2).sum(axis=1)))\n",
    "    communalities = 1.0 - uniqueness\n",
    "    comm_df = pd.DataFrame({\"communality\": communalities, \"uniqueness\": uniqueness}, index=features)\n",
    "\n",
    "    # Save artifacts\n",
    "    loadings_csv = out_dir / f\"efa_{label}_loadings.csv\"\n",
    "    loadings_df.to_csv(loadings_csv)\n",
    "    print(f\"[{label}] Saved loadings to {loadings_csv}\")\n",
    "\n",
    "    comm_csv = out_dir / f\"efa_{label}_communalities.csv\"\n",
    "    comm_df.to_csv(comm_csv)\n",
    "    print(f\"[{label}] Saved communalities/uniqueness to {comm_csv}\")\n",
    "\n",
    "    # Heatmap of loadings\n",
    "    plt.figure(figsize=(max(6, 0.5 * loadings_df.shape[1] + 4), max(6, 0.25 * loadings_df.shape[0] + 2)))\n",
    "    sns.heatmap(loadings_df, cmap=\"coolwarm\", center=0, cbar_kws={\"shrink\": 0.6})\n",
    "    plt.title(f\"{label.upper()} Factor Loadings ({n_factors} factors)\")\n",
    "    plt.tight_layout()\n",
    "    loadings_png = out_dir / f\"efa_{label}_loadings.png\"\n",
    "    plt.savefig(loadings_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[{label}] Saved loadings heatmap to {loadings_png}\")\n",
    "\n",
    "    return {\n",
    "        \"n_factors\": n_factors,\n",
    "        \"loadings\": loadings_df,\n",
    "        \"communalities\": comm_df,\n",
    "        \"scree\": scree_path,\n",
    "    }\n",
    "\n",
    "# Apply correlation filter with confirmed parameters, then EFA\n",
    "CF_METHOD = \"spearman\"\n",
    "CF_THRESHOLD = 0.95\n",
    "\n",
    "def run_correlation_filter_and_efa(df: pd.DataFrame, label: str):\n",
    "    # Correlation filter\n",
    "    filtered, dropped, corr_kept = correlation_filter(df, method=CF_METHOD, threshold=CF_THRESHOLD)\n",
    "    # Save correlation after filtering\n",
    "    out_csv = out_dir / f\"corr_{label}_filtered_{CF_METHOD}.csv\"\n",
    "    corr_kept.to_csv(out_csv)\n",
    "    print(f\"[{label}] Saved filtered correlation matrix to {out_csv}\")\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(max(8, min(20, 0.35 * corr_kept.shape[1])), max(6, min(20, 0.35 * corr_kept.shape[0]))))\n",
    "    sns.heatmap(corr_kept, cmap=\"vlag\", center=0, square=True, cbar_kws={\"shrink\": 0.6}, linewidths=0.3)\n",
    "    plt.title(f\"{label.upper()} — filtered ({CF_METHOD}) correlation\")\n",
    "    plt.tight_layout()\n",
    "    img_path = out_dir / f\"corr_{label}_filtered_{CF_METHOD}.png\"\n",
    "    plt.savefig(img_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[{label}] Saved filtered correlation heatmap to {img_path}\")\n",
    "    if dropped:\n",
    "        print(f\"[{label}] Dropped due to high correlation (|r|>={CF_THRESHOLD}): {', '.join(dropped)}\")\n",
    "\n",
    "    # EFA on filtered data\n",
    "    run_efa(filtered, label=label, out_dir=out_dir, rotation=\"varimax\", kaiser=True, n_factors=None)\n",
    "\n",
    "run_correlation_filter_and_efa(df_mmse_only, label=\"mmse_only\")\n",
    "run_correlation_filter_and_efa(df_moca_only, label=\"moca_only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
