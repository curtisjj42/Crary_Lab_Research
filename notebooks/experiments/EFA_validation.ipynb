{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries and data files.\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Define the output directory and file paths\n",
    "out_dir = Path('../../outputs/uds_extraction')\n",
    "file_path = out_dir / 'moca_only.parquet'\n",
    "\n",
    "# Check if the file exists\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist. Please ensure it is generated first.\")\n",
    "\n",
    "# Load cleaned DataFrame from existing work or relevant source path.\n",
    "cleaned_df = pd.read_parquet(file_path)\n",
    "\n",
    "# Select the specified columns for the new dataset.\n",
    "\"\"\"# Set 1\n",
    "moca_columns = [\n",
    "    'DIGFORCT', 'DIGFORSL', 'DIGBACCT', 'DIGBACLS', 'MINTTOTS', 'MINTTOTW', 'MINTSCNG', 'MINTSCNC',\n",
    "    'MINTPCNG', 'MINTPCNC'\n",
    "]\"\"\"\n",
    "\n",
    "\"\"\"# Set 2\n",
    "moca_columns = [\n",
    "    'CRAFTVRS', 'CRAFTURS', 'CRAFTDVR', 'CRAFTDRE', 'CRAFTDTI', 'CRAFTCUE',\n",
    "    'MOCATRAI', 'MOCACUBE', 'MOCACLOC', 'MOCACLON', 'MOCACLOH'\n",
    "]\"\"\"\n",
    "\n",
    "# Set 3\n",
    "moca_columns = [\n",
    "    'MINTTOTS', 'ANIMALS', 'VEG',\n",
    "    'TRAILA', 'MOCACUBE', 'UDSBENTC'\n",
    "]\n",
    "\n",
    "# Create a modified MOCA-only DataFrame with selected columns.\n",
    "modified_moca_only_df = cleaned_df[moca_columns].copy()\n",
    "# Save the raw selection before any filtering\n",
    "raw_selected_parquet = out_dir / 'moca_only_selected.parquet'\n",
    "modified_moca_only_df.to_parquet(raw_selected_parquet)\n",
    "print(f\"Saved initial selected MOCA variables to: {raw_selected_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "# Note: Rotation helpers removed; using FactorAnalyzer with built-in 'promax' rotation\n",
    "\n",
    "\n",
    "def clean_and_run_efa(df, label, out_dir):\n",
    "    \"\"\"Clean data and run EFA with multiple rotation methods\"\"\"\n",
    "    print(f\"\\n=== Running EFA for {label} ===\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # Remove rows with any missing values\n",
    "    df_clean = df.dropna()\n",
    "    print(f\"After removing missing values: {df_clean.shape}\")\n",
    "\n",
    "    # Check for and remove constant columns (zero variance)\n",
    "    constant_cols = []\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].std() == 0:\n",
    "            constant_cols.append(col)\n",
    "\n",
    "    if constant_cols:\n",
    "        print(f\"Removing {len(constant_cols)} constant columns: {constant_cols}\")\n",
    "        df_clean = df_clean.drop(columns=constant_cols)\n",
    "\n",
    "    if df_clean.shape[1] < 2:\n",
    "        print(f\"Error: Not enough variables ({df_clean.shape[1]}) for EFA after removing constant columns\")\n",
    "        return\n",
    "\n",
    "    print(f\"Final shape for analysis: {df_clean.shape}\")\n",
    "\n",
    "    # Standardize the data\n",
    "    Z = (df_clean - df_clean.mean()) / df_clean.std(ddof=0)\n",
    "\n",
    "    # Verify standardization worked - convert to numeric and check\n",
    "    try:\n",
    "        Z_numeric = Z.apply(pd.to_numeric, errors='coerce')\n",
    "        if Z.isnull().any().any() or np.isinf(Z_numeric.values).any():\n",
    "            print(\"Error: Standardization produced NaN or infinite values\")\n",
    "            return\n",
    "    except Exception:\n",
    "        # If conversion fails, just check for NaN\n",
    "        if Z.isnull().any().any():\n",
    "            print(\"Error: Standardization produced NaN values\")\n",
    "            return\n",
    "\n",
    "    # Check for zero variance columns before correlation calculation\n",
    "    variance_check = Z.var()\n",
    "    zero_var_cols = variance_check[variance_check == 0].index.tolist()\n",
    "\n",
    "    if zero_var_cols:\n",
    "        print(f\"Warning: Removing {len(zero_var_cols)} zero-variance columns: {zero_var_cols}\")\n",
    "        Z = Z.drop(columns=zero_var_cols)\n",
    "    \n",
    "    # Also check for near-zero variance (numerical stability)\n",
    "    near_zero_var_cols = variance_check[variance_check < 1e-10].index.tolist()\n",
    "    if near_zero_var_cols:\n",
    "        print(f\"Warning: Removing {len(near_zero_var_cols)} near-zero variance columns: {near_zero_var_cols}\")\n",
    "        Z = Z.drop(columns=near_zero_var_cols)\n",
    "\n",
    "    # Check if we have enough columns left for EFA\n",
    "    if Z.shape[1] < 3:\n",
    "        print(f\"Error: Only {Z.shape[1]} variables remaining after variance check. EFA requires at least 3 variables.\")\n",
    "        return\n",
    "\n",
    "    # Now safely calculate correlation matrix\n",
    "    try:\n",
    "        corr = np.corrcoef(Z.values, rowvar=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating correlation matrix: {e}\")\n",
    "        # Alternative: use pandas correlation which is more robust\n",
    "        corr = Z.corr().values\n",
    "\n",
    "    # Determine number of factors via Kaiser criterion (eigenvalues > 1)\n",
    "    try:\n",
    "        eigvals = np.linalg.eigvalsh(corr)\n",
    "    except np.linalg.LinAlgError:\n",
    "        eigvals = np.linalg.eigvals(corr).real\n",
    "    n_factors = int((eigvals > 1.0).sum())\n",
    "    n_factors = max(1, min(n_factors, Z.shape[1]))\n",
    "\n",
    "    # Fit FactorAnalyzer with promax rotation\n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation='promax')\n",
    "    fa.fit(Z.values)\n",
    "\n",
    "    loadings = fa.loadings_  # variables x factors\n",
    "    features = list(df.columns)\n",
    "    loadings_df = pd.DataFrame(loadings, index=features, columns=[f\"F{i+1}\" for i in range(loadings.shape[1])])\n",
    "    try:\n",
    "        communalities = pd.Series(fa.get_communalities(), index=features)\n",
    "    except Exception:\n",
    "        communalities = pd.Series((loadings ** 2).sum(axis=1), index=features)\n",
    "\n",
    "    rot_name = \"promax\"\n",
    "\n",
    "    # Save artifacts\n",
    "    loadings_csv = out_dir / f\"efa_{label}_{rot_name}_loadings.csv\"\n",
    "    loadings_df.to_csv(loadings_csv)\n",
    "    print(f\"[{label} | {rot_name}] Saved loadings to {loadings_csv}\")\n",
    "\n",
    "    comm_csv = out_dir / f\"efa_{label}_{rot_name}_communalities.csv\"\n",
    "    pd.DataFrame({\"communality\": communalities}, index=features).to_csv(comm_csv)\n",
    "    print(f\"[{label} | {rot_name}] Saved communalities to {comm_csv}\")\n",
    "\n",
    "    # Plot and save heatmap of factors vs. variable loadings\n",
    "    try:\n",
    "        # Dynamic figure size based on matrix shape\n",
    "        n_vars, n_factors = loadings_df.shape\n",
    "        width = max(6, min(18, 1.2 * n_factors + 3))\n",
    "        height = max(6, min(24, 0.35 * n_vars + 3))\n",
    "\n",
    "        plt.figure(figsize=(width, height))\n",
    "        sns.heatmap(\n",
    "            loadings_df,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            linewidths=0.5,\n",
    "            linecolor=\"#f0f0f0\",\n",
    "            cbar_kws={\"label\": \"Loading\"}\n",
    "        )\n",
    "        plt.title(f\"EFA Loadings Heatmap — {label} ({rot_name})\")\n",
    "        plt.xlabel(\"Factors\")\n",
    "        plt.ylabel(\"Variables\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        heatmap_png = out_dir / f\"efa_{label}_{rot_name}_loadings_heatmap.png\"\n",
    "        plt.savefig(heatmap_png, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[{label} | {rot_name}] Saved loadings heatmap to {heatmap_png}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{label} | {rot_name}] Failed to create heatmap: {e}\")\n",
    "\n",
    "# Add this code before line 70 to create the reduced_moca_df\n",
    "# Remove highly correlated variables to create a de-collinearized dataset\n",
    "correlation_matrix = modified_moca_only_df.corr().abs()\n",
    "\n",
    "# Find pairs of variables with high correlation (threshold = 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if correlation_matrix.iloc[i, j] > 0.9:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
    "\n",
    "# Remove one variable from each highly correlated pair\n",
    "variables_to_remove = set()\n",
    "for var1, var2 in high_corr_pairs:\n",
    "    # Keep the variable with higher variance (or you could use other criteria)\n",
    "    if modified_moca_only_df[var1].var() < modified_moca_only_df[var2].var():\n",
    "        variables_to_remove.add(var1)\n",
    "    else:\n",
    "        variables_to_remove.add(var2)\n",
    "\n",
    "# Create the reduced dataset\n",
    "reduced_moca_df = modified_moca_only_df.drop(columns=list(variables_to_remove))\n",
    "\n",
    "print(f\"Removed {len(variables_to_remove)} highly correlated variables: {list(variables_to_remove)}\")\n",
    "print(f\"Reduced dataset shape: {reduced_moca_df.shape}\")\n",
    "\n",
    "clean_and_run_efa(modified_moca_only_df, \"moca_only\", out_dir)\n",
    "clean_and_run_efa(reduced_moca_df, \"moca_only_reduced\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_vif(df):\n",
    "    # Select only numeric columns and drop any non-numeric data\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Remove columns with all NaN or constant values\n",
    "    numeric_df = numeric_df.dropna(axis=1, how='all')\n",
    "    numeric_df = numeric_df.loc[:, numeric_df.nunique() > 1]\n",
    "    \n",
    "    # Remove rows with any NaN values (listwise deletion)\n",
    "    X = numeric_df.dropna(axis=0, how='any')\n",
    "    \n",
    "    if X.shape[1] == 0:\n",
    "        return pd.DataFrame(columns=[\"Feature\", \"VIF\"])\n",
    "    \n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        print(f\"Warning: More variables ({X.shape[1]}) than observations ({X.shape[0]}). VIF may be unreliable.\")\n",
    "    \n",
    "    # Ensure all data is float type to avoid type issues\n",
    "    X = X.astype(float)\n",
    "    \n",
    "    try:\n",
    "        vif_vals = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "        return pd.DataFrame({\"Feature\": X.columns, \"VIF\": vif_vals}).sort_values(\"VIF\", ascending=False).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating VIF: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Feature\", \"VIF\"])\n",
    "\n",
    "\n",
    "def reduce_multicollinearity(df, threshold=10.0):\n",
    "    \"\"\"Iteratively drop variables with highest VIF above threshold.\n",
    "    Returns (reduced_df, dropped_features, final_vif_df)\n",
    "    dropped_features is a list of tuples (feature, vif_at_drop)\n",
    "    \"\"\"\n",
    "    X = df.select_dtypes(include=[np.number]).copy()\n",
    "    X = X.dropna(axis=1, how='all')\n",
    "    X = X.loc[:, X.nunique() > 1]\n",
    "    dropped = []\n",
    "    while True:\n",
    "        vif_df = calculate_vif(X)\n",
    "        if vif_df.empty:\n",
    "            break\n",
    "        max_row = vif_df.iloc[0]\n",
    "        if float(max_row[\"VIF\"]) <= threshold:\n",
    "            break\n",
    "        feat_to_drop = str(max_row[\"Feature\"])\n",
    "        dropped.append((feat_to_drop, float(max_row[\"VIF\"])) )\n",
    "        X = X.drop(columns=[feat_to_drop])\n",
    "        if X.shape[1] < 2:\n",
    "            break\n",
    "    final_vif = calculate_vif(X)\n",
    "    return X, dropped, final_vif\n",
    "\n",
    "# Run VIF analysis on the selected MOCA features\n",
    "print(\"[multicollinearity] Calculating VIF on selected MOCA variables…\")\n",
    "vif_initial = calculate_vif(modified_moca_only_df)\n",
    "vif_csv = out_dir / 'efa_moca_only_vif.csv'\n",
    "vif_initial.to_csv(vif_csv, index=False)\n",
    "\n",
    "# Reduce multicollinearity by removing highly collinear tests\n",
    "reduced_moca_df, dropped_features, vif_final = reduce_multicollinearity(modified_moca_only_df, threshold=10.0)\n",
    "vif_final_csv = out_dir / 'efa_moca_only_vif_reduced.csv'\n",
    "vif_final.to_csv(vif_final_csv, index=False)\n",
    "print(f\"Saved reduced VIF table to: {vif_final_csv}\")\n",
    "\n",
    "if dropped_features:\n",
    "    print(\"[multicollinearity] Dropped the following redundant tests (feature, VIF when dropped):\")\n",
    "    for feat, val in dropped_features:\n",
    "        print(f\"  - {feat}: {val:.2f}\")\n",
    "else:\n",
    "    print(\"[multicollinearity] No features exceeded the VIF threshold; none dropped.\")\n",
    "\n",
    "# Save reduced dataset\n",
    "reduced_parquet = out_dir / 'moca_only_selected_reduced.parquet'\n",
    "reduced_moca_df.to_parquet(reduced_parquet)\n",
    "print(f\"Saved reduced MOCA variables to: {reduced_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
